{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#  mplfinance Date iLoc Transform\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows multiple outputs from a single jupyter notebook cell:\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.12.7a18'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mplfinance as mpf\n",
    "mpf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/SP500_NOV2019_Hist.csv',index_col=0,parse_dates=True)\n",
    "df = pd.read_csv('../data/yahoofinance-SPY-20200901-20210113.csv',index_col=0,parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uses for DateIlocTransform\n",
    "\n",
    "#### All uses are for `show_nontrading=False` only:\n",
    "\n",
    "- `.to_date()` &nbsp; location *to* date: &nbsp; for tick label formatting.\n",
    "- `.to_iloc()` &nbsp; date *to* location: &nbsp; for `xticks` placement.\n",
    "- `.to_iloc()` &nbsp; date *to* location: &nbsp; for `xlim` placement.\n",
    "- `.to_iloc()` &nbsp; date *to* location: &nbsp; for `lines` placement.\n",
    "\n",
    "---\n",
    "\n",
    "- It seems to me that  \n",
    "  - **interpolation** may be better using the actual datetime series (rather than the linear formula), whereas \n",
    "  - **extrapolation** *will require* the linear formula.\n",
    "    - Or for \"known\" cases may be able to use **date calculations**, for example:\n",
    "      - quartile(0.65) == quartile(0.50) == quartile(0.35) == \"known\" frequency.\n",
    "      - intraday with consistent trading hours in data\n",
    "      - daily with weekends missing (maybe someday allow users to supply holidays)\n",
    "      - weekly, monthly, yearly, etc. are simple?\n",
    "  - need to run some tests to see which, if either, is better.\n",
    "- Keep in mind, while testing, that `xlim` values will affect `xticks` placement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-09-01</th>\n",
       "      <td>350.209991</td>\n",
       "      <td>352.709991</td>\n",
       "      <td>349.239990</td>\n",
       "      <td>352.600006</td>\n",
       "      <td>349.703522</td>\n",
       "      <td>54999300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-02</th>\n",
       "      <td>354.670013</td>\n",
       "      <td>358.750000</td>\n",
       "      <td>353.429993</td>\n",
       "      <td>357.700012</td>\n",
       "      <td>354.761627</td>\n",
       "      <td>69540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-03</th>\n",
       "      <td>355.869995</td>\n",
       "      <td>356.380005</td>\n",
       "      <td>342.589996</td>\n",
       "      <td>345.390015</td>\n",
       "      <td>342.552765</td>\n",
       "      <td>148011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-04</th>\n",
       "      <td>346.130005</td>\n",
       "      <td>347.829987</td>\n",
       "      <td>334.869995</td>\n",
       "      <td>342.570007</td>\n",
       "      <td>339.755890</td>\n",
       "      <td>139156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-08</th>\n",
       "      <td>336.709991</td>\n",
       "      <td>342.640015</td>\n",
       "      <td>332.880005</td>\n",
       "      <td>333.209991</td>\n",
       "      <td>330.472778</td>\n",
       "      <td>114465300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2020-09-01  350.209991  352.709991  349.239990  352.600006  349.703522   \n",
       "2020-09-02  354.670013  358.750000  353.429993  357.700012  354.761627   \n",
       "2020-09-03  355.869995  356.380005  342.589996  345.390015  342.552765   \n",
       "2020-09-04  346.130005  347.829987  334.869995  342.570007  339.755890   \n",
       "2020-09-08  336.709991  342.640015  332.880005  333.209991  330.472778   \n",
       "\n",
       "               Volume  \n",
       "Date                   \n",
       "2020-09-01   54999300  \n",
       "2020-09-02   69540000  \n",
       "2020-09-03  148011100  \n",
       "2020-09-04  139156300  \n",
       "2020-09-08  114465300  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-06</th>\n",
       "      <td>369.709991</td>\n",
       "      <td>376.980011</td>\n",
       "      <td>369.119995</td>\n",
       "      <td>373.549988</td>\n",
       "      <td>373.549988</td>\n",
       "      <td>107997700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-07</th>\n",
       "      <td>376.100006</td>\n",
       "      <td>379.899994</td>\n",
       "      <td>375.910004</td>\n",
       "      <td>379.100006</td>\n",
       "      <td>379.100006</td>\n",
       "      <td>68766800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-08</th>\n",
       "      <td>380.589996</td>\n",
       "      <td>381.489990</td>\n",
       "      <td>377.100006</td>\n",
       "      <td>381.260010</td>\n",
       "      <td>381.260010</td>\n",
       "      <td>71677200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-11</th>\n",
       "      <td>377.850006</td>\n",
       "      <td>380.579987</td>\n",
       "      <td>377.720001</td>\n",
       "      <td>378.690002</td>\n",
       "      <td>378.690002</td>\n",
       "      <td>51176700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-12</th>\n",
       "      <td>378.890015</td>\n",
       "      <td>379.859985</td>\n",
       "      <td>376.359985</td>\n",
       "      <td>378.769989</td>\n",
       "      <td>378.769989</td>\n",
       "      <td>52445000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2021-01-06  369.709991  376.980011  369.119995  373.549988  373.549988   \n",
       "2021-01-07  376.100006  379.899994  375.910004  379.100006  379.100006   \n",
       "2021-01-08  380.589996  381.489990  377.100006  381.260010  381.260010   \n",
       "2021-01-11  377.850006  380.579987  377.720001  378.690002  378.690002   \n",
       "2021-01-12  378.890015  379.859985  376.359985  378.769989  378.769989   \n",
       "\n",
       "               Volume  \n",
       "Date                   \n",
       "2021-01-06  107997700  \n",
       "2021-01-07   68766800  \n",
       "2021-01-08   71677200  \n",
       "2021-01-11   51176700  \n",
       "2021-01-12   52445000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "df.head()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.indexes.datetimes.DatetimeIndex"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pandas._libs.tslibs.timestamps.Timestamp"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "numpy.datetime64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-01 00:00:00\n",
      "2020-09-01T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "s = df.head().index.to_series()\n",
    "type(s.index)\n",
    "type(s.index[0])\n",
    "type(s.values)\n",
    "type(s.values[0])\n",
    "s.index[0] == s.values[0]\n",
    "print(s.index[0])\n",
    "print(s.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cases\n",
    "- For each use case (data entered) we interpolate a few points, and extrapolate a few points, both datetime -> iloc and iloc -> datetime\n",
    "\n",
    "1. Intraday, part of a day (1 to 4 hours).  Data frequencies: min, 3min, 15min, 30min, hour, 90 min\n",
    "2. Intraday, one full day.  (Data freq: min, 3min, 15min, 30min, hour, 90 min)\n",
    "3. Intraday, 1.5 days. ...\n",
    "4. Intraday, 10 days (skip weekends; weekend=FS and weekend=SS)\n",
    "5. Intraday, 10 days (including weekends; weekend=FS and weekend=SS)\n",
    "6. Daily, 3 days\n",
    "7. Daily, 45 days (skip weekends, 1-day weekends, no weekends, holidays)\n",
    "8. Daily, 125 days (skip weekends, 1-day weekends, no weekends, holidays)\n",
    "9. Weekly ...\n",
    "10. Monthly ...\n",
    "11. Quarterly ...\n",
    "12. Yearly ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ditransform as ditf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_extrapolation_limits\n",
      "_inferred_frequency\n",
      "_to_date_series\n",
      "_to_iloc_series\n",
      "infer_frequency\n",
      "infer_weekmask\n",
      "time_series_index\n",
      "timedelta_to_freqabbr\n",
      "to_datetime\n",
      "to_iloc\n"
     ]
    }
   ],
   "source": [
    "tf = ditf.DateIlocTransform(df.index)\n",
    "\n",
    "dl = dir(tf)\n",
    "for item in dl:\n",
    "    if item[0:2] != '__':\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../data/*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/SP500_20191106_IDayBollinger.csv',\n",
       " '../data/SP500_NOV2019_Hist.csv',\n",
       " '../data/SP500_NOV2019_IDay.csv',\n",
       " '../data/SP500_NOV2019_IDayRVol.csv',\n",
       " '../data/SPY_20110701_20120630_Bollinger.csv',\n",
       " '../data/jpyusd_barchartdotcom.csv',\n",
       " '../data/yahoofinance-AAPL-20040819-20180120.csv',\n",
       " '../data/yahoofinance-GOOG-20040819-20180120.csv',\n",
       " '../data/yahoofinance-INTC-19950101-20040412.csv',\n",
       " '../data/yahoofinance-SPY-20080101-20180101.csv',\n",
       " '../data/yahoofinance-SPY-20200901-20210113.csv',\n",
       " '../data/SP500_20191106_IDayBollTweak.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2019-11-06 09:30:00   2019-11-06 09:30:00\n",
      "2019-11-06 09:31:00   2019-11-06 09:31:00\n",
      "2019-11-06 09:32:00   2019-11-06 09:32:00\n",
      "2019-11-06 15:58:00   2019-11-06 15:58:00\n",
      "2019-11-06 15:59:00   2019-11-06 15:59:00\n",
      "2019-11-06 16:00:00   2019-11-06 16:00:00\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "=== ../data/SP500_20191106_IDayBollinger.csv ===\n",
      "TimeSeriesFrequency(d='B', i='1T', w='None', o='09:30:00', c='16:00:00')\n",
      "\n",
      "=== ../data/SP500_NOV2019_Hist.csv ===\n",
      "TimeSeriesFrequency(d='B', i='None', w='None', o='09:30:00', c='16:00:00')\n",
      "\n",
      "Date\n",
      "2019-11-05 09:30:00   2019-11-05 09:30:00\n",
      "2019-11-05 09:31:00   2019-11-05 09:31:00\n",
      "2019-11-05 09:32:00   2019-11-05 09:32:00\n",
      "2019-11-08 15:57:00   2019-11-08 15:57:00\n",
      "2019-11-08 15:58:00   2019-11-08 15:58:00\n",
      "2019-11-08 15:59:00   2019-11-08 15:59:00\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "=== ../data/SP500_NOV2019_IDay.csv ===\n",
      "TimeSeriesFrequency(d='B', i='1T', w='None', o='09:30:00', c='16:00:00')\n",
      "\n",
      "Date\n",
      "2019-11-05 09:30:00   2019-11-05 09:30:00\n",
      "2019-11-05 09:31:00   2019-11-05 09:31:00\n",
      "2019-11-05 09:32:00   2019-11-05 09:32:00\n",
      "2019-11-08 15:57:00   2019-11-08 15:57:00\n",
      "2019-11-08 15:58:00   2019-11-08 15:58:00\n",
      "2019-11-08 15:59:00   2019-11-08 15:59:00\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "=== ../data/SP500_NOV2019_IDayRVol.csv ===\n",
      "TimeSeriesFrequency(d='B', i='1T', w='None', o='09:30:00', c='16:00:00')\n",
      "\n",
      "=== ../data/SPY_20110701_20120630_Bollinger.csv ===\n",
      "TimeSeriesFrequency(d='B', i='None', w='None', o='09:30:00', c='16:00:00')\n",
      "\n",
      "=== ../data/jpyusd_barchartdotcom.csv ===\n",
      "TimeSeriesFrequency(d='B', i='None', w='None', o='09:30:00', c='16:00:00')\n",
      "\n",
      "=== ../data/yahoofinance-AAPL-20040819-20180120.csv ===\n",
      "TimeSeriesFrequency(d='B', i='None', w='None', o='09:30:00', c='16:00:00')\n",
      "\n",
      "=== ../data/yahoofinance-GOOG-20040819-20180120.csv ===\n",
      "TimeSeriesFrequency(d='B', i='None', w='None', o='09:30:00', c='16:00:00')\n",
      "\n",
      "=== ../data/yahoofinance-INTC-19950101-20040412.csv ===\n",
      "TimeSeriesFrequency(d='B', i='None', w='None', o='09:30:00', c='16:00:00')\n",
      "\n",
      "=== ../data/yahoofinance-SPY-20080101-20180101.csv ===\n",
      "TimeSeriesFrequency(d='B', i='None', w='None', o='09:30:00', c='16:00:00')\n",
      "\n",
      "=== ../data/yahoofinance-SPY-20200901-20210113.csv ===\n",
      "TimeSeriesFrequency(d='B', i='None', w='None', o='09:30:00', c='16:00:00')\n",
      "\n",
      "Date\n",
      "2019-11-06 09:45:00   2019-11-06 09:45:00\n",
      "2019-11-06 09:46:00   2019-11-06 09:46:00\n",
      "2019-11-06 09:47:00   2019-11-06 09:47:00\n",
      "2019-11-06 15:58:00   2019-11-06 15:58:00\n",
      "2019-11-06 15:59:00   2019-11-06 15:59:00\n",
      "2019-11-06 16:00:00   2019-11-06 16:00:00\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "=== ../data/SP500_20191106_IDayBollTweak.csv ===\n",
      "TimeSeriesFrequency(d='B', i='1T', w='None', o='09:30:00', c='16:00:00')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fn in files:\n",
    "    df = pd.read_csv(fn,index_col=0,parse_dates=True)\n",
    "    tf = ditf.DateIlocTransform(df.index)\n",
    "    \n",
    "    print('===',fn,'===')\n",
    "    print(tf._inferred_frequency)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ../data/SP500_20191106_IDayBollinger.csv ===\n",
      "weekmask= [False, False, True, False, False, False, False] True= 1\n",
      "\n",
      "=== ../data/SP500_NOV2019_Hist.csv ===\n",
      "weekmask= [True, True, True, True, True, False, False] True= 5\n",
      "\n",
      "=== ../data/SP500_NOV2019_IDay.csv ===\n",
      "weekmask= [False, True, True, True, True, False, False] True= 4\n",
      "\n",
      "=== ../data/SP500_NOV2019_IDayRVol.csv ===\n",
      "weekmask= [False, True, True, True, True, False, False] True= 4\n",
      "\n",
      "=== ../data/SPY_20110701_20120630_Bollinger.csv ===\n",
      "weekmask= [True, True, True, True, True, False, False] True= 5\n",
      "\n",
      "=== ../data/jpyusd_barchartdotcom.csv ===\n",
      "weekmask= [True, True, True, True, True, False, False] True= 5\n",
      "\n",
      "=== ../data/yahoofinance-AAPL-20040819-20180120.csv ===\n",
      "weekmask= [True, True, True, True, True, False, False] True= 5\n",
      "\n",
      "=== ../data/yahoofinance-GOOG-20040819-20180120.csv ===\n",
      "weekmask= [True, True, True, True, True, False, False] True= 5\n",
      "\n",
      "=== ../data/yahoofinance-INTC-19950101-20040412.csv ===\n",
      "weekmask= [True, True, True, True, True, False, False] True= 5\n",
      "\n",
      "=== ../data/yahoofinance-SPY-20080101-20180101.csv ===\n",
      "weekmask= [True, True, True, True, True, False, False] True= 5\n",
      "\n",
      "=== ../data/yahoofinance-SPY-20200901-20210113.csv ===\n",
      "weekmask= [True, True, True, True, True, False, False] True= 5\n",
      "\n",
      "=== ../data/SP500_20191106_IDayBollTweak.csv ===\n",
      "weekmask= [False, False, True, False, False, False, False] True= 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fn in files:\n",
    "    df = pd.read_csv(fn,index_col=0,parse_dates=True)\n",
    "    #tf = ditf.DateIlocTransform(df.index)\n",
    "    wm = ditf.DateIlocTransform.infer_weekmask(df.index)    \n",
    "    print('===',fn,'===')\n",
    "    print('weekmask=',wm, 'True=',wm.count(True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wm.count(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-05 09:30:00</th>\n",
       "      <td>3080.80</td>\n",
       "      <td>3080.49</td>\n",
       "      <td>3081.47</td>\n",
       "      <td>3080.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-05 09:31:00</th>\n",
       "      <td>3080.33</td>\n",
       "      <td>3079.36</td>\n",
       "      <td>3080.33</td>\n",
       "      <td>3079.15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-05 09:32:00</th>\n",
       "      <td>3079.43</td>\n",
       "      <td>3079.68</td>\n",
       "      <td>3080.46</td>\n",
       "      <td>3079.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-05 09:33:00</th>\n",
       "      <td>3079.73</td>\n",
       "      <td>3079.15</td>\n",
       "      <td>3080.22</td>\n",
       "      <td>3079.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-05 09:34:00</th>\n",
       "      <td>3079.20</td>\n",
       "      <td>3079.62</td>\n",
       "      <td>3080.03</td>\n",
       "      <td>3079.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-08 15:55:00</th>\n",
       "      <td>3090.80</td>\n",
       "      <td>3090.70</td>\n",
       "      <td>3090.80</td>\n",
       "      <td>3089.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-08 15:56:00</th>\n",
       "      <td>3090.69</td>\n",
       "      <td>3090.74</td>\n",
       "      <td>3090.76</td>\n",
       "      <td>3090.05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-08 15:57:00</th>\n",
       "      <td>3090.73</td>\n",
       "      <td>3090.70</td>\n",
       "      <td>3091.02</td>\n",
       "      <td>3090.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-08 15:58:00</th>\n",
       "      <td>3090.73</td>\n",
       "      <td>3091.04</td>\n",
       "      <td>3091.13</td>\n",
       "      <td>3090.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-08 15:59:00</th>\n",
       "      <td>3091.16</td>\n",
       "      <td>3092.91</td>\n",
       "      <td>3092.91</td>\n",
       "      <td>3090.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1563 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Open    Close     High      Low  Volume\n",
       "Date                                                           \n",
       "2019-11-05 09:30:00  3080.80  3080.49  3081.47  3080.30       0\n",
       "2019-11-05 09:31:00  3080.33  3079.36  3080.33  3079.15       0\n",
       "2019-11-05 09:32:00  3079.43  3079.68  3080.46  3079.43       0\n",
       "2019-11-05 09:33:00  3079.73  3079.15  3080.22  3079.12       0\n",
       "2019-11-05 09:34:00  3079.20  3079.62  3080.03  3079.07       0\n",
       "...                      ...      ...      ...      ...     ...\n",
       "2019-11-08 15:55:00  3090.80  3090.70  3090.80  3089.99       0\n",
       "2019-11-08 15:56:00  3090.69  3090.74  3090.76  3090.05       0\n",
       "2019-11-08 15:57:00  3090.73  3090.70  3091.02  3090.52       0\n",
       "2019-11-08 15:58:00  3090.73  3091.04  3091.13  3090.58       0\n",
       "2019-11-08 15:59:00  3091.16  3092.91  3092.91  3090.96       0\n",
       "\n",
       "[1563 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = '../data/SP500_NOV2019_IDay.csv'\n",
    "df = pd.read_csv(fn,index_col=0,parse_dates=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsf = ditf.TimeSeriesFrequency(dfreq='B',ifreq='T',weekmask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeSeriesFrequency(d='B', i='T', w='None', o='09:30:00', c='16:00:00')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-05-27 10:00:00')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = pd.Timestamp('5/27/21 10')\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`start` must be less than `end`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/bb/data/tmp/ipykernel_101236/246268265.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m ix = ditf.DateIlocTransform.time_series_index(start=pd.Timestamp('05/27/2021 10:02'),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                               \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'05/27/2021 0:01'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                               freq=tsf)#'05/27/21','06/07/21',freq=tsf)\n",
      "\u001b[0;32m/bb/mbigd/mbig5/mpf/mplfinance/examples/scratch_pad/ditransform.py\u001b[0m in \u001b[0;36mtime_series_index\u001b[0;34m(start, end, freq)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`end` must be of type pd.Timestamp, or type str parseable to pd.Timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`start` must be less than `end`.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mbday_freq\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdfreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: `start` must be less than `end`."
     ]
    }
   ],
   "source": [
    "ix = ditf.DateIlocTransform.time_series_index(start=pd.Timestamp('05/27/2021 10:02'),\n",
    "                                              end='05/27/2021 0:01',\n",
    "                                              freq=tsf)#'05/27/21','06/07/21',freq=tsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ix)\n",
    "len(ix)/60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2021-05-27 09:30:00', '2021-05-27 09:31:00',\n",
       "               '2021-05-27 09:32:00', '2021-05-27 09:33:00',\n",
       "               '2021-05-27 09:34:00', '2021-05-27 09:35:00',\n",
       "               '2021-05-27 09:36:00', '2021-05-27 09:37:00',\n",
       "               '2021-05-27 09:38:00', '2021-05-27 09:39:00',\n",
       "               '2021-05-27 09:40:00', '2021-05-27 09:41:00',\n",
       "               '2021-05-27 09:42:00', '2021-05-27 09:43:00',\n",
       "               '2021-05-27 09:44:00', '2021-05-27 09:45:00',\n",
       "               '2021-05-27 09:46:00', '2021-05-27 09:47:00',\n",
       "               '2021-05-27 09:48:00', '2021-05-27 09:49:00',\n",
       "               '2021-05-27 09:50:00', '2021-05-27 09:51:00',\n",
       "               '2021-05-27 09:52:00', '2021-05-27 09:53:00',\n",
       "               '2021-05-27 09:54:00', '2021-05-27 09:55:00',\n",
       "               '2021-05-27 09:56:00', '2021-05-27 09:57:00',\n",
       "               '2021-05-27 09:58:00', '2021-05-27 09:59:00',\n",
       "               '2021-05-27 10:00:00', '2021-05-27 10:01:00'],\n",
       "              dtype='datetime64[ns]', freq='T')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3031286301.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/bb/data/tmp/ipykernel_101236/3031286301.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    STOP HERE\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "STOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.Timestamp('09:30')\n",
    "t2 = pd.Timestamp('09:29')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.time() > t2.time()\n",
    "t1.time() == t2.time()\n",
    "t1.time() < t2.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t1.time().minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSF:\n",
    "    \n",
    "    def __init__(self, topen):\n",
    "        self._topen = pd.Timestamp(topen)\n",
    "        \n",
    "    @property\n",
    "    def topen(self):\n",
    "        return self._topen\n",
    "\n",
    "#     @topen.setter\n",
    "#     def topen(self,new_topen):\n",
    "#         #print('new_topen',new_topen)\n",
    "#         self._topen = pd.Timestamp(new_topen)   \n",
    "#         print('type(self._topen)=',type(self._topen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsf = TSF('09:30')\n",
    "tsf.topen = '09:31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tsf.topen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsf.topen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ix)\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wm = tf.infer_weekmask(df.index)\n",
    "wm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsf = tf.infer_frequency(df.index)\n",
    "tsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ditransform import DateIlocTransform as DIT\n",
    "tf = DIT(df.index)\n",
    "dl = dir(tf)\n",
    "for item in dl:\n",
    "    if item[0:2] != '__':\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsf = ditf.TimeSeriesFrequency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ditf.DateIlocTransform(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = t.infer_frequency(df.index)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixtemp = pd.DatetimeIndex([df.index[j] for j in range(0,len(df.index),2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = tf.infer_frequency(ixtemp)\n",
    "ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ixt = tf.time_series_index(start=df.index[0],end=df.index[-1],freq=f)\n",
    "ixt.name = 'Generated'\n",
    "s1 = df.index.to_series()\n",
    "s2 = ixt.to_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf = pd.DataFrame([df.index.to_series(),ixt.to_series()]).T\n",
    "len(xdf)\n",
    "select = xdf[xdf.isna().any(axis=1)]\n",
    "select\n",
    "s1[select.index]\n",
    "#s2[select.index]\n",
    "xdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClass:\n",
    "    abc = 12.0\n",
    "    def __init__(self):\n",
    "        self.xyz = 2.0\n",
    "        \n",
    "    @classmethod\n",
    "    def smed(cls,p):\n",
    "        print('abc=',cls.abc)\n",
    "        print('  p=',p)\n",
    "        \n",
    "    @staticmethod\n",
    "    def m1(x):\n",
    "        print('m1: x=',x)\n",
    "        \n",
    "    @staticmethod\n",
    "    def m2(y):\n",
    "        print('m2: y=',y)\n",
    "        c = MyClass\n",
    "        c.m1(y)\n",
    "        \n",
    "    @classmethod\n",
    "    def cm1(cls,x):\n",
    "        print('m1: x=',x)\n",
    "        \n",
    "    @classmethod\n",
    "    def cm2(cls,y):\n",
    "        print('m2: y=',y)\n",
    "        cls.m1(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = MyClass()\n",
    "c.xyz\n",
    "c.abc\n",
    "c.smed(99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.m2(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.cm2(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf.iloc[40:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(t)\n",
    "t._inferred_frequency\n",
    "t._to_date_series.head(3)\n",
    "t._to_date_series.tail(3)\n",
    "t._to_iloc_series.head(3)\n",
    "t._to_iloc_series.tail(3)\n",
    "t.to_datetime\n",
    "t.to_iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[11:15]\n",
    "for ix in np.arange(12,14,0.1):\n",
    "    d = t.to_datetime(ix)\n",
    "    print(\"%3.1f   %s   %3.1f\" % (ix,d,t.to_iloc(d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.index[-4:-1]\n",
    "for ix in np.arange(90.5,93,0.1):\n",
    "    d = t.to_datetime(ix)\n",
    "    print(\"%3.1f   %s\" % (ix,d))\n",
    "    ixchk = t.to_iloc(d)\n",
    "    print(\"%3.1f   %s   %3.1f\" % (ix,d,ixchk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filelist = glob.glob('../data/*.csv')\n",
    "for fn in filelist:\n",
    "    df = pd.read_csv(fn,index_col=0,parse_dates=True)\n",
    "    f = infer_frequency(df)\n",
    "    print(f,fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn ='../data/yahoofinance-INTC-19950101-20040412.csv'\n",
    "fn='../data/gbpusd_yf20210401-0407.csv'\n",
    "#fn='../data/SP500_20191106_IDayBollinger.csv'\n",
    "#fn = '../data/jpyusd_barchartdotcom.csv'\n",
    "df = pd.read_csv(fn,index_col=0,parse_dates=True)\n",
    "infer_frequency(df[::-1],trace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggdict = {'Open'  :'first',\n",
    "           'High'  :'max',\n",
    "           'Low'   :'min',\n",
    "           'Close' :'last',\n",
    "           'Volume':'sum'\n",
    "          }\n",
    "fn ='../data/yahoofinance-INTC-19950101-20040412.csv'\n",
    "df = pd.read_csv(fn,index_col=0,parse_dates=True)\n",
    "\n",
    "\n",
    "for rs in ['1D','2D','3D','4D','5D','6D','7D','8D','9D','1W','2W','1M','2M','3M','4M','1Q','1Y']:\n",
    "    f = infer_frequency(df.resample(rs).agg(aggdict))\n",
    "    print('rs=',rs,'  f=',f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filelist = glob.glob('../data/*.csv')\n",
    "count = 0\n",
    "import sys\n",
    "for fn in filelist:\n",
    "    df = pd.read_csv(fn,index_col=0,parse_dates=True)\n",
    "    f  = infer_frequency(df)\n",
    "    ix = time_series_index(start=df.index[0],end=df.index[-1],freq=f)\n",
    "    print(ix[[0,1,2,-3,-2,1]])\n",
    "    \n",
    "    so = df.index.to_series(name='Original')\n",
    "    sg = ix.to_series(name='Generated')\n",
    "    comp = pd.merge(so,sg,left_index=True,right_index=True,how='outer')\n",
    "    comp.to_csv('comp.csv')\n",
    "    #print('comp[comp.isnull()]=')\n",
    "    #isn = comp[comp.isnull()]\n",
    "    print(comp.head(18))\n",
    "    print(comp.tail(18))\n",
    "    try:\n",
    "        sd = ix == df.index\n",
    "        sd = pd.Series(sd)\n",
    "        print(sd.value_counts())\n",
    "    except:\n",
    "        e = sys.exc_info()[1]\n",
    "        print(type(e),'('+str(e)+')')\n",
    "        print('generated=',len(ix),' original=',len(df.index))\n",
    "        print('original df.index range:',df.index[0],df.index[-1])        \n",
    "    print(f,fn,'\\n')\n",
    "    count += 1\n",
    "    if count > 0:\n",
    "        print('break')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wkdf = pd.DataFrame(dict(date=ix,dayofweek=[d.dayofweek for d in ix]))\n",
    "wkdf.head(4)\n",
    "\n",
    "weekdays = wkdf.groupby('dayofweek').count()\n",
    "weekdays\n",
    "weekdays.iloc[:,0].mean()\n",
    "weekdays.iloc[:,0].max()\n",
    "\n",
    "len(weekdays)\n",
    "\n",
    "cut = weekdays.max()[0] - 10\n",
    "len(weekdays[ weekdays['date'] > cut])\n",
    "\n",
    "cut = weekdays.max()[0] / 2\n",
    "len(weekdays[ weekdays['date'] > cut])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = '5/1/2021'\n",
    "end = '5/31/2021'\n",
    "#pd.bdate_range(start=start,end=end,freq=bday_freq,weekmask=weekmask)\n",
    "mask=[0,1,2,3,4]\n",
    "\n",
    "weekmask = [day in mask for day in range(7)]\n",
    "weekmask\n",
    "ix1 = pd.bdate_range(start=start,end=end,freq='C',weekmask=weekmask)\n",
    "\n",
    "weekmask='Mon Tue Wed Thu Fri'\n",
    "ix2 = pd.bdate_range(start=start,end=end,freq='C',weekmask=weekmask)\n",
    "\n",
    "all(ix1 == ix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def _lsq_linear(self,dtseries):\n",
    "#         '''\n",
    "#         Calculate `y = mx + b` linear relationship between `date` and\n",
    "#         `iloc` in `dtseries`.  Return slope (m) and y_intercept (b).\n",
    "#         This closed-form linear least squares algorithm was taken from\n",
    "#         https://mmas.github.io/least-squares-fitting-numpy-scipy\n",
    "#         '''\n",
    "#         si = dtseries\n",
    "#         s  = si.dropna() \n",
    "#         if len(s) < 2:\n",
    "#             err = 'NOT enough data for Least Squares'\n",
    "#             if (len(si) > 2):\n",
    "#                 err += ', due to presence of NaNs'\n",
    "#             raise ValueError(err)\n",
    "#         xs = mdates.date2num(s.index.to_pydatetime())\n",
    "#         ys = [y for y in range(len(xs))]\n",
    "#         a  = np.vstack([xs, np.ones(len(xs))]).T\n",
    "#         m, b  = np.dot(np.linalg.inv(np.dot(a.T,a)), np.dot(a.T,ys))\n",
    "#         #x1, x2 = xs[0], xs[-1]\n",
    "#         #y1 = m*x1 + b\n",
    "#         #y2 = m*x2 + b\n",
    "#         #x1, x2 = mdates.num2date(x1), mdates.num2date(x2)\n",
    "#         #return ((x1,y1),(x2,y2))\n",
    "#         return m, b\n",
    "    \n",
    "#     def _ep_linear(self,dtseries):\n",
    "#         d1 = _date_to_mdate(dtseries.index[0])\n",
    "#         d2 = _date_to_mdate(dtseries.index[-1])\n",
    "\n",
    "#         i1 = 0.0\n",
    "#         i2 = len(dtseries) - 1.0\n",
    "\n",
    "#         slope   = (i2 - i1) / (d2 - d1)\n",
    "#         yitrcpt1 = i1 - (slope*d1)\n",
    "#         yitrcpt2 = i2 - (slope*d2)\n",
    "#         if yitrcpt1 != yitrcpt2:\n",
    "#             print('WARNING: yintercepts NOT equal!!!(',yitrcpt1,yitrcpt2,')')\n",
    "#             yitrcpt = (yitrcpt1 + yitrcpt2) / 2.0\n",
    "#         else:\n",
    "#             yitrcpt = yitrcpt1 \n",
    "#         return slope, yitrcpt            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Timestamp.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit wkday(ix)\n",
    "%timeit dofwk(ix)\n",
    "%timeit wkday(ix)\n",
    "%timeit dofwk(ix)\n",
    "\n",
    "\n",
    "%timeit dofwk(ix)\n",
    "%timeit wkday(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.index)\n",
    "len(ix)\n",
    "all(df.index == ix)\n",
    "sd = pd.Series(df.index == ix)\n",
    "sd.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[388:393]\n",
    "ix[388:393]\n",
    "df.iloc[779:784]\n",
    "ix[779:784]\n",
    "df.iloc[-4:]\n",
    "ix[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Some experiments with Pandas inferred frequency:\n",
    "#### As soon as a Holiday date is missing, Pandas inferred frequency fails<br> (only allows regular non-business days, but not holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/yahoofinance-SPY-20200901-20210113.csv',index_col=0,parse_dates=True)\n",
    "df = pd.read_csv('../data/SP500_NOV2019_Hist.csv',index_col=0,parse_dates=True)\n",
    "df.index\n",
    "len(df)\n",
    "df.index[6:18].inferred_freq\n",
    "df.index[6:18]\n",
    "df.index[0:20].inferred_freq\n",
    "df.index[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "bday_us = pd.offsets.CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "\n",
    "dtixBH = pd.bdate_range('11/1/2019','11/30/2019',freq=bday_us)\n",
    "#dtixBH\n",
    "\n",
    "dtixB  = pd.bdate_range('11/1/2019','11/30/2019',freq='B')\n",
    "#dtixB\n",
    "\n",
    "dtixD  = pd.bdate_range('11/1/2019','11/30/2019',freq='D')\n",
    "#dtixD\n",
    "\n",
    "#is1 = dtixB.intersection(dtixBH)\n",
    "#is2 = dtixBH.intersection(dtixB)\n",
    "#is1\n",
    "#is1 == is2\n",
    "len(dtixBH)\n",
    "len(dtixB)\n",
    "len(dtixD)\n",
    "\n",
    "\n",
    "#dtixBH.to_series()[is1] == dtixBH.to_series()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sBH = dtixBH.to_series(name='BH')\n",
    "sB  = dtixB.to_series(name='B')\n",
    "sD  = dtixD.to_series(name='D')\n",
    "\n",
    "df  = pd.merge(sB,sBH,left_index=True,right_index=True,how='outer')\n",
    "df  = pd.merge(df,sD,left_index=True,right_index=True,how='outer')\n",
    "\n",
    "df\n",
    "len(sB)\n",
    "len(sBH)\n",
    "len(sD)\n",
    "len(df)\n",
    "for col in df.columns:\n",
    "    print('===',col,'===')\n",
    "    pd.value_counts([ isinstance(d,pd.Timestamp) for d in df[col] ])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts([ isinstance(d,pd.Timestamp) for d in df['BH'] ])\n",
    "pd.value_counts([ isinstance(d,pd.Timestamp) for d in df['B'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtix  = pd.bdate_range('11/1/2019','11/3/2019',freq='T')\n",
    "dtix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "data = [25. + random.random()*25. for j in range(len(dtix))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(data,index=dtix)\n",
    "s\n",
    "#s.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = s.resample('15T').ohlc()\n",
    "sr\n",
    "#sr.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [d.weekday() for d in pd.Series([d.date() for d in sr.index]).unique()]\n",
    "# [d.weekday() for d in pd.Series([d.date() for d in sBH.index]).unique()]\n",
    "\n",
    "[d.strftime('%a') for d in pd.Series([d.date() for d in sr.index]).unique()]\n",
    "days = [d.strftime('%a') for d in pd.Series([d.date() for d in sBH.index]).unique()]\n",
    "#days\n",
    "import calendar\n",
    "print(list(calendar.day_abbr))\n",
    "for day in calendar.day_abbr:\n",
    "    print(day,day in days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d.strftime('%a') for d in pd.Series([d.date() for d in sr.index]).unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(sr.index.values)\n",
    "ix = pd.Index(a)\n",
    "print('ix.inferred_freq=',str(ix.inferred_freq))\n",
    "del a[100]\n",
    "ix = pd.Index(a)\n",
    "print('ix.inferred_freq=',str(ix.inferred_freq))\n",
    "\n",
    "print('ix[0:99].inferred_freq=',str(ix[0:99].inferred_freq))\n",
    "\n",
    "print('ix[99:].inferred_freq=',str(ix[99:].inferred_freq))\n",
    "\n",
    "print('ix[100:].inferred_freq=',str(ix[100:].inferred_freq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix.inferred_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "list(calendar.day_name)\n",
    "list(calendar.day_abbr)\n",
    "print(calendar.calendar(2021))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('===  infer_frequency(df.head(4))  ===')\n",
    "infer_frequency(df.head(4))\n",
    "print('===  infer_frequency(df.head(6).tail(2))  ===')\n",
    "infer_frequency(df.head(6).tail(2))\n",
    "print('===  infer_frequency(df.head(3))  ===')\n",
    "infer_frequency(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design:\n",
    "\n",
    "- It now seems to me, that if we can identify one of the common patterns (listed below) then we can generate datetimes with that pattern to provide a ***more accurate extrapolation*** outside of the range of datetime data provided.\n",
    "  - if we cannot identify a pattern ***then*** we should fall back on linear extrapolation\n",
    "- The common patterns that we will attempt to find are:\n",
    "  - intraday, **one date**, with a regular frequency (hour, minutes, etc.)\n",
    "  - intraday, **multiple dates**, with a regular frequency (hour, minutes, etc.), ***and*** possibly skipping over weekends.\n",
    "    - For both of the above **intraday** case (especially the multiple date case) trading only between Open and Close times.\n",
    "  - daily, 5-day trading week, Mon-Fri\n",
    "  - daily, 5-day trading week, Sun-Thu\n",
    "  - daily, 6-day trading week ??\n",
    "  - weekly  (every Monday, or every Friday)\n",
    "  - Monthly (1st of month or last of month)\n",
    "  - Quaterly (1st of quarter or last of quarter)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Experiments\n",
    "\n",
    "- extrapolation: least squares versus end-to-end linear\n",
    "  - intraday 1m, 15m, 1h, 3h\n",
    "  - daily M-F\n",
    "  - weekly\n",
    "  \n",
    "- extrapolation: known frequency formula\n",
    "  - intraday 1m, 15m, 1h, 3h\n",
    "  - daily M-F\n",
    "  - weekly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -l ../data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = ['../data/SP500_NOV2019_Hist.csv',\n",
    "         '../data/SP500_NOV2019_IDayRVol.csv',\n",
    "         '../data/SPY_20110701_20120630_Bollinger.csv',\n",
    "         '../data/yahoofinance-GOOG-20040819-20180120.csv',\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with diff value counts in search of a good frequency-inference algorithm\n",
    "\n",
    "- Given a series S, then the following give identical results:\n",
    "  - `diff = (S.shift(-1) - S).shift(1)`\n",
    "  - `diff = S.diff(1)`\n",
    "- The following also give identical results, with each other:\n",
    "  - `diff = S.diff(-1)`\n",
    "  - `diff = (S.shift(1) - S).shift(-1)`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for file in INPUT:\n",
    "    data = pd.read_csv(file,index_col=0,parse_dates=True)\n",
    "    data.iloc[[0,1,-1],:].style.set_caption(str(data.shape)+' '+file)\n",
    "    dts  = data.index.to_series()\n",
    "    diff = dts.diff(1)\n",
    "    diff.value_counts()\n",
    "    diff.value_counts().idxmax()\n",
    "    diff.value_counts().index[0]\n",
    "    diff.value_counts().idxmax() == diff.value_counts().index[0]\n",
    "    type(diff.value_counts())\n",
    "    diff.value_counts().index\n",
    "    min(diff.value_counts().index)\n",
    "    min(diff.value_counts().index) == diff.value_counts().idxmax()\n",
    "    type(diff.value_counts().index[0])\n",
    "    td = diff.value_counts().keys()[0]\n",
    "    td.days,td.seconds,td.microseconds,td.nanoseconds\n",
    "    type(td.days),type(td.seconds),type(td.microseconds),type(td.nanoseconds)\n",
    "    #for jj in range(9,0,-1):\n",
    "    #    q = round(0.1*jj,1)\n",
    "    #    print('diff.quantile('+str(q)+')=',diff.quantile(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doff = pd.offsets.DateOffset(minutes=1)\n",
    "doff\n",
    "doff.freqstr\n",
    "#dir(doff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import matplotlib.dates as mdates\n",
    "from mplfinance._utils import _date_to_mdate\n",
    "    \n",
    "class DateIlocTransform:\n",
    "    '''Create a transform object that can transform from a date to a DatetimeIndex location, and vis versa.\n",
    "    Requires a Pandas DatetimeIndex upon creation\n",
    "    If `date` does not exactly match a date in the series then interpolate between two dates.\n",
    "    If `date` is outside the range of dates in the series, then extrapolate.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,dtindex):\n",
    "        if not isinstance(dtindex,pd.DatetimeIndex):\n",
    "            raise TypeError('Need `pandas.DatetimeIndex`, but got \"'+str(type(dtindex))+'\"')\n",
    "        if not len(dtindex) > 1:\n",
    "            raise ValueError('`dtindex` must have length of at least 2.')\n",
    "        ixlist = np.linspace(0,len(dtindex),len(dtindex)+1)[0:-1]\n",
    "        self._to_iloc_series = pd.Series(ixlist,index=dtindex)\n",
    "        self._to_date_series = pd.Series(dtindex.values,index=ixlist)\n",
    "        dtseries = dtindex.to_series()\n",
    "        self._lsslope, self._lsyicpt = self._lsq_linear(dtseries)\n",
    "        self._epslope, self._epyicpt = self._ep_linear(dtseries)\n",
    "        \n",
    "\n",
    "    def _lsq_linear(self,dtseries):\n",
    "        '''\n",
    "        Calculate `y = mx + b` linear relationship between `date` and\n",
    "        `iloc` in `dtseries`.  Return slope (m) and y_intercept (b).\n",
    "        This closed-form linear least squares algorithm was taken from\n",
    "        https://mmas.github.io/least-squares-fitting-numpy-scipy\n",
    "        '''\n",
    "        si = dtseries\n",
    "        s  = si.dropna() \n",
    "        if len(s) < 2:\n",
    "            err = 'NOT enough data for Least Squares'\n",
    "            if (len(si) > 2):\n",
    "                err += ', due to presence of NaNs'\n",
    "            raise ValueError(err)\n",
    "        xs = mdates.date2num(s.index.to_pydatetime())\n",
    "        ys = [y for y in range(len(xs))]\n",
    "        a  = np.vstack([xs, np.ones(len(xs))]).T\n",
    "        m, b  = np.dot(np.linalg.inv(np.dot(a.T,a)), np.dot(a.T,ys))\n",
    "        #x1, x2 = xs[0], xs[-1]\n",
    "        #y1 = m*x1 + b\n",
    "        #y2 = m*x2 + b\n",
    "        #x1, x2 = mdates.num2date(x1), mdates.num2date(x2)\n",
    "        #return ((x1,y1),(x2,y2))\n",
    "        return m, b\n",
    "    \n",
    "    def _ep_linear(self,dtseries):\n",
    "        d1 = _date_to_mdate(dtseries.index[0])\n",
    "        d2 = _date_to_mdate(dtseries.index[-1])\n",
    "\n",
    "        i1 = 0.0\n",
    "        i2 = len(dtseries) - 1.0\n",
    "\n",
    "        slope   = (i2 - i1) / (d2 - d1)\n",
    "        yitrcpt1 = i1 - (slope*d1)\n",
    "        yitrcpt2 = i2 - (slope*d2)\n",
    "        if yitrcpt1 != yitrcpt2:\n",
    "            print('WARNING: yintercepts NOT equal!!!(',yitrcpt1,yitrcpt2,')')\n",
    "            yitrcpt = (yitrcpt1 + yitrcpt2) / 2.0\n",
    "        else:\n",
    "            yitrcpt = yitrcpt1 \n",
    "        return slope, yitrcpt\n",
    "    \n",
    "    def to_iloc(self,date,method='ls'):\n",
    "        if method == 'ls':   # Least Squares linear\n",
    "            return self._lsslope*mdates.date2num(date) + self._lsyicpt\n",
    "        elif method == 'ep': # End Point linear\n",
    "            return self._epslope*mdates.date2num(date) + self._epyicpt\n",
    "        elif method == 'in': # INterpolate\n",
    "            #self._to_iloc_series = pd.Series(range(len(dtindex)),index=dtindex)\n",
    "            i1s = self._to_iloc_series.loc[:date]\n",
    "            i1  = i1s[-1] if len(i1s) > 0 else float('nan') # else need to extrapolate\n",
    "            i2s = self._to_iloc_series.loc[date:]\n",
    "            i2  = i2s[ 0] if len(i2s) > 0 else float('nan') # else need to extrapolate\n",
    "            #print('\\ndate,i1,i2=',date,i1,i2)\n",
    "            loc1 = i1\n",
    "            loc2 = i2\n",
    "            d1 = self._to_date_series.iloc[int(round(i1,0))]\n",
    "            d2 = self._to_date_series.iloc[int(round(i2,0))]\n",
    "            #print('date,i1,i2,d1,d2=',date,i1,i2,d1,d2)\n",
    "            loc = ((date-d1)/(d2-d1))*(loc2-loc1) + loc1 if d1 != d2 else loc1\n",
    "            #print('loc1,loc2,loc=',loc1,loc2,loc)\n",
    "            return loc\n",
    "        else:\n",
    "            raise ValueError('Bad value for `method`: ('+str(method)+') ')\n",
    "        return loc2\n",
    "    \n",
    "    def to_datetime(self,iloc,method='ls'):\n",
    "        '''\n",
    "        y = mx + b    \n",
    "        x = (y-b)/m\n",
    "        '''\n",
    "        if method == 'ls':    # Least Squares linear\n",
    "            d = (iloc - self._lsyicpt)/self._lsslope\n",
    "            return mdates.num2date(d).replace(tzinfo=None)\n",
    "        elif method == 'ep':  # End Point linear\n",
    "            d = (iloc - self._epyicpt)/self._epslope\n",
    "            return mdates.num2date(d).replace(tzinfo=None)\n",
    "        elif method == 'in': # INterpolate\n",
    "            #self._to_date_series = pd.Series(dtindex.values,index=range(len(dtindex))) \n",
    "            d1s = self._to_date_series.loc[:iloc]\n",
    "            d1  = d1s.iloc[-1] if len(d1s) > 0 else float('nan') # else should extrapolate\n",
    "            d2s = self._to_date_series.loc[iloc:]\n",
    "            d2  = d2s.iloc[ 0] if len(d2s) > 0 else float('nan') # else should extrapolate\n",
    "            if d1 == d2:\n",
    "                return d1\n",
    "            loc1 = int(round(self._to_iloc_series.loc[d1],0))\n",
    "            loc2 = int(round(self._to_iloc_series.loc[d2],0))\n",
    "            #print('\\nd1,d2=',d1.date().day,d2.date().day,\n",
    "            #      ' iloc,loc1,loc2=',iloc,loc1,loc2)\n",
    "            #print('\\nd1,d2,(d2-d1),type(d2-d1)=',d1,d2,d2-d1,type(d2-d1))\n",
    "            # d1,d2= 8 11  iloc,loc1,loc2= 5.333333333333333 5 6\n",
    "            # d1,d2,(d2-d1)= 2019-11-08 00:00:00 2019-11-11 00:00:00 3 days 00:00:00\n",
    "            # Timestamp('2019-11-08 23:59:59.999999999')\n",
    "            # d = ((iloc-loc1)/(loc2-loc1))*(d2-d1) + d1\n",
    "            d = ((iloc-loc1)*(d2-d1)) + d1\n",
    "            return d.round('s')\n",
    "        else:\n",
    "            raise ValueError('Bad value for `method`: ('+str(method)+') ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in INPUT:\n",
    "    data = pd.read_csv(file,index_col=0,parse_dates=True)\n",
    "    data.iloc[[0,1,-1],:].style.set_caption(str(data.shape)+' '+file)\n",
    "    print(infer_frequency(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggdict = {'Open'  :'first',\n",
    "           'High'  :'max',\n",
    "           'Low'   :'min',\n",
    "           'Close' :'last',\n",
    "           'Volume':'sum'\n",
    "          }\n",
    "f = infer_frequency(data.resample('1W').agg(aggdict))\n",
    "f\n",
    "pd.Timedelta(days=7)\n",
    "f == pd.Timedelta(days=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.resample('1M').agg(aggdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_frequency(data.resample('1M').agg(aggdict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Timedelta('1Month')\n",
    "d1 = data.loc['2004-10',:]\n",
    "d2 = data.resample('1M').agg(aggdict)\n",
    "d3 = data.asfreq('1M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.iloc[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we are going to develop a transform similar to date_to_iloc() with the following features:\n",
    "- Able to transform both directions\n",
    "- saves the relavant input data to avoid *some* recalculation\n",
    "- implementation as a class will enable saving the input data in the transform object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/SP500_NOV2019_IDayRVol.csv',index_col=0,parse_dates=True)\n",
    "dtindex = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtseries = dtindex.to_series()\n",
    "dtseries['2019-11-08 15:50:01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtseries.describe(datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = dtseries.shift(-1) - dtseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.quantile(0.9981)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Here we experiment with finding gaps in the data, specifically for the example of *intraday* data with specific trading hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../../tmp.py\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# Construct dummy dataframe\n",
    "dates = pd.to_datetime([\n",
    "    '2016-08-03',\n",
    "    '2016-08-04',\n",
    "    '2016-08-05',\n",
    "    '2016-08-17',\n",
    "    '2016-09-05',\n",
    "    '2016-09-06',\n",
    "    '2016-09-07',\n",
    "    '2016-09-19'])\n",
    "df = pd.DataFrame(dates, columns=['date'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the diff of the first column (drop 1st row since it's undefined)\n",
    "deltas = df['date'].diff()[1:]\n",
    "deltas\n",
    "# Filter diffs (here days > 1, but could be seconds, hours, etc)\n",
    "gaps = deltas[deltas > timedelta(days=1)]\n",
    "gaps\n",
    "# Print results\n",
    "#print(f'{len(gaps)} gaps with average gap duration: {gaps.mean()}')\n",
    "#for i, g in gaps.iteritems():\n",
    "#    gap_start = df['date'][i - 1]\n",
    "#    print(f'Start: {datetime.strftime(gap_start, \"%Y-%m-%d\")} | '\n",
    "#          f'Duration: {str(g.to_pytimedelta())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = pd.read_csv('../data/SP500_NOV2019_IDayRVol.csv',index_col=0,parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf.iloc[[0,1,2,-2,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = idf.index.to_series().diff()[1:]\n",
    "deltas\n",
    "# Filter diffs (here days > 1, but could be seconds, hours, etc)\n",
    "deltas.value_counts()\n",
    "freq = deltas.value_counts().idxmax()\n",
    "freq\n",
    "gaps = deltas[deltas != freq]\n",
    "gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf.head(3)\n",
    "idf.loc['2019-11-06'].index[-1]\n",
    "idf.loc['2019-11-07'].index[ 0]\n",
    "idf.loc['2019-11-07'].index[ 0] - idf.loc['2019-11-06'].index[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The datetime index associate with the gap (i.e. with the gap duration)<br> appears to be the ***end time*** of the gap (or rather the start time of the next *non-gap*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/SP500_NOV2019_Hist.csv',index_col=0,parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dit = DateIlocTransform(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtindex = df.index\n",
    "ixlist = np.linspace(0,len(dtindex),len(dtindex)+1)[0:-1]\n",
    "to_iloc_series = pd.Series(ixlist,index=dtindex)\n",
    "to_date_series = pd.Series(dtindex.values,index=ixlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['ep','ls']:\n",
    "    print('\\n=== method: \"'+method+'\"  =====')\n",
    "    for d in df.index.to_pydatetime():\n",
    "        il = dit.to_iloc(d,method=method)\n",
    "        dt = dit.to_datetime(il,method=method)\n",
    "        err = 'ERR' if d.date().day != dt.date().day else ''\n",
    "        print(\"%6.2f  %2d  %2d  %3s\" % (il,d.date().day,dt.date().day,err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[0:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "daterange = pd.date_range(start=df.index[0],end=df.index[-1],freq='D')\n",
    "#for d in df.index.to_pydatetime():\n",
    "print('len(daterange)=',len(daterange))\n",
    "for d in daterange:\n",
    "    il = dit.to_iloc(d,method='in')\n",
    "    #print('il=',il,type(il))\n",
    "    dt = dit.to_datetime(il,method='in')\n",
    "    err = 'ERR' if d.date().day != dt.date().day else ''\n",
    "    print(\"%6.2f  %s  %2d  %2d  %3s\" % (il,d.date(),d.date().day,dt.date().day,err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = 0.333333333333333*pd.Timedelta(days=3)\n",
    "td\n",
    "td.round('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[0]\n",
    "df.index[-1] - df.index[0]\n",
    "(df.index[-1] - df.index[0]).days\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(df))/(df.index[-1] - df.index[0]).days\n",
    "(len(df)-1)/(df.index[-1] - df.index[0]).days\n",
    "5/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[19]\n",
    "df.index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\n==== INTC ====')\n",
    "# df = pd.read_csv('../data/yahoofinance-INTC-19950101-20040412.csv',index_col=0,parse_dates=True)\n",
    "# df.iloc[[0,1,-2,-1]]\n",
    "# len(df)\n",
    "# print('\\n==== GOOG ====')\n",
    "df = pd.read_csv('../data/yahoofinance-GOOG-20040819-20180120.csv',index_col=0,parse_dates=True)\n",
    "df.iloc[[0,1,-2,-1]]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.index[-1] - df.index[0]).days\n",
    "(len(df))/(df.index[-1] - df.index[0]).days\n",
    "(len(df)-1)/(df.index[-1] - df.index[0]).days\n",
    "5/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.index[-1] - df.index[0]).days\n",
    "yrs = ((df.index[-1] - df.index[0]).days - 1) / 365.25\n",
    "hldys = yrs*3\n",
    "((5/7)*365.25 - hldys)/365.25\n",
    "print()\n",
    "4/7\n",
    "6/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(df)-1)/(df.index[-1]-df.index[0]).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = df.index.to_series().diff(1)\n",
    "dvc = diff.value_counts()\n",
    "dvc[0]/dvc[1]\n",
    "dvc\n",
    "dvc.keys()\n",
    "dne1 = diff[ diff > pd.Timedelta(days=1) ]\n",
    "len(diff)\n",
    "len(dne1)\n",
    "vc = dne1.value_counts()\n",
    "vc\n",
    "vc[0]/vc[1]\n",
    "vc[0]/vc[1] > 2\n",
    "vc.idxmax().days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = pd.date_range(start='1/1/2001',periods=30,freq='D')\n",
    "dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(dr)-1)/(dr[-1] - dr[0]).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = pd.date_range(start='1/1/2001',periods=30,freq='B')\n",
    "dr\n",
    "\n",
    "bday_Israel = pd.offsets.CustomBusinessDay(weekmask='Sun Mon Tue Wed Thu')\n",
    "drI = pd.date_range(start='1/1/2001',periods=30,freq=bday_Israel)\n",
    "drI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2f = pd.offsets.CustomBusinessDay(weekmask='Mon Tue Wed Thu Fri')\n",
    "drm2f = pd.date_range(start='1/1/2001',periods=30,freq=m2f)\n",
    "drm2f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(dr == drm2f)\n",
    "all(dr == drI)\n",
    "dr == drI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drt = pd.date_range(dr[0],dr[-1],freq='B')\n",
    "drt\n",
    "dr == drt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = pd.date_range('1/1/2021',periods=12,freq='BM')\n",
    "dr\n",
    "dr = pd.date_range('1/1/2021',periods=12,freq='W-FRI')\n",
    "dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dr)\n",
    "dr[-1] - dr[0]\n",
    "(len(dr)-1) == (dr[-1]-dr[0]).days/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idr = pd.date_range('1/2/2021 09:30',periods=12,freq='15T')\n",
    "idr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T15  = pd.offsets.Minute(15)\n",
    "BD   = pd.offsets.BDay()\n",
    "TH   = pd.offsets.BusinessHour(start='09:30',end='16:00')\n",
    "do15 = pd.offsets.DateOffset(minutes=15)\n",
    "do15 == T15\n",
    "T15\n",
    "BD\n",
    "TH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.bdate_range('1/1/2021','1/13/2021',freq='15min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.bdate_range('1/1/2021','1/13/2021',freq='B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "bday_us = pd.offsets.CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "pd.bdate_range('1/1/2021','1/13/2021',freq=bday_us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bday_me = pd.offsets.CustomBusinessDay(weekmask='Sun Mon Tue Wed Thu')\n",
    "pd.bdate_range('1/1/2021','1/13/2021',freq=bday_me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bhours = pd.offsets.BusinessHour(start='09:30',end='16:00')\n",
    "pd.bdate_range('1/1/2021','1/13/2021',freq=bhours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bday_us = pd.offsets.CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "dtindex = pd.bdate_range('1/1/2021','1/13/2021',freq=bday_us)\n",
    "ixlist  = []\n",
    "for dt in dtindex:\n",
    "    d1 = dt.replace(hour=9,minute=30)\n",
    "    d2 = dt.replace(hour=16,minute=1) # make sure to include 16:00\n",
    "    # Here we must use `date_range()` instead of `bdate_range()`\n",
    "    ixlist.append(pd.date_range(d1,d2,freq='30min'))\n",
    "trading_index = ixlist[0].union_many(ixlist[1:])\n",
    "print(trading_index[range(16)])\n",
    "print('...')\n",
    "print(trading_index[range(63,77)])\n",
    "print('...')\n",
    "print(trading_index[range(-16,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bday_us = pd.offsets.CustomBusinessDay(calendar=USFederalHolidayCalendar())\n",
    "dtindex = pd.bdate_range('1/1/2021','1/13/2021',freq=bday_us)\n",
    "octimes = [] # open and close times\n",
    "for dt in dtindex:\n",
    "    octimes.append(dt.replace(hour=9,minute=30))\n",
    "    octimes.append(dt.replace(hour=16,minute=0))\n",
    "\n",
    "for ts in octimes:\n",
    "    print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idr = pd.date_range('1/2/2021 09:30',periods=500,freq='15T')\n",
    "#idr\n",
    "t_hours = idr[idr.indexer_between_time(start_time='09:30',end_time='16:00')]\n",
    "t_hours[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.Series(range(len(idr)),index=idr)\n",
    "ts.index[ts.index.indexer_between_time(start_time='09:30',end_time='16:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = pd.bdate_range('01/01/2021 09:30','01/13/2021 16:00',freq='D')\n",
    "dr\n",
    "dr = pd.bdate_range('01/01/2021 09:30','01/13/2021 16:00',freq='B')\n",
    "dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dr = pd.date_range('01/01/2021 09:30','01/13/2021 16:00',freq=BD)\n",
    "dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr[0]\n",
    "dr[0].replace(hour=16,minute=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for d in dr:\n",
    "    d2 = d.replace(hour=16,minute=1)\n",
    "    indexes.append(pd.date_range(d,d2,freq='15T'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = indexes[0]\n",
    "ix = ix.union_many(indexes[1:])\n",
    "ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for d in ix[0:200]:\n",
    "# for d in ix[20:32]:\n",
    "#    print(d)\n",
    "ix[20:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_time = '16:00'\n",
    "close = pd.Timestamp(close_time)\n",
    "close\n",
    "close.hour\n",
    "close.minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Alias|Description|\n",
    "|---|---|\n",
    "|B    |business day frequency|\n",
    "|C    |custom business day frequency|\n",
    "|D    |calendar day frequency|\n",
    "|W    |weekly frequency|\n",
    "|M    |month end frequency|\n",
    "|SM   |semi-month end frequency (15th and end of month)|\n",
    "|BM   |business month end frequency|\n",
    "|CBM  |custom business month end frequency|\n",
    "|MS   |month start frequency|\n",
    "|SMS  |semi-month start frequency (1st and 15th)|\n",
    "|BMS  |business month start frequency|\n",
    "|CBMS  |custom business month start frequency|\n",
    "|Q     |quarter end frequency|\n",
    "|BQ    |business quarter end frequency|\n",
    "|QS    |quarter start frequency|\n",
    "|BQS   |business quarter start frequency|\n",
    "|A,Y   |year end frequency|\n",
    "|BA,BY |business year end frequency|\n",
    "|AS,YS |year start frequency|\n",
    "|BAS,BYS | business year start frequency|\n",
    "|BH    |business hour frequency|\n",
    "|H     |hourly frequency|\n",
    "|T,min |minutely frequency|\n",
    "\n",
    "|S     |secondly frequency|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.bdate_range(start='1/1/2021',end='12/31/2022',freq='M',weekmask=None)\n",
    "pd.bdate_range(start='1/1/2021',end='12/31/2022',freq='BM',weekmask=None)\n",
    "pd.bdate_range(start='1/1/2021',end='12/31/2022',freq='MS',weekmask=None)\n",
    "pd.bdate_range(start='1/1/2021',end='12/31/2022',freq='BMS',weekmask=None)\n",
    "\n",
    "pd.bdate_range(start='1/1/2021',end='12/31/2022',freq='Q',weekmask=None)\n",
    "pd.bdate_range(start='1/1/2021',end='12/31/2022',freq='BQ',weekmask=None)\n",
    "pd.bdate_range(start='1/1/2021',end='12/31/2022',freq='QS',weekmask=None)\n",
    "pd.bdate_range(start='1/1/2021',end='12/31/2022',freq='BQS',weekmask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.offsets.CustomBusinessDay(calendar=USFederalHolidayCalendar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfreq = TimeSeriesFrequency(dfreq=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.calendar.holidays\n",
    "d.calendar.weekmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_day_range(bday_start=None,bday_end=None,bday_freq='B',\n",
    "                      open_time='09:30',close_time='16:00',iday_freq='15T',weekmask=None):\n",
    "\n",
    "    if bday_start is None: bday_start = pd.Timestamp.today()\n",
    "    if bday_end   is None: bday_end = bday_start + pd.Timedelta(days=1)\n",
    "\n",
    "    daily = []\n",
    "    for d in pd.bdate_range(start=bday_start,end=bday_end,freq=bday_freq,weekmask=weekmask):\n",
    "        topen  = pd.Timestamp(open_time)\n",
    "        d1     = d.replace(hour=topen.hour,minute=topen.minute)\n",
    "        tclose = pd.Timestamp(close_time)\n",
    "        d2     = d.replace(hour=tclose.hour,minute=tclose.minute+1)\n",
    "        daily.append(pd.date_range(d1,d2,freq=iday_freq))\n",
    "   \n",
    "    index = daily[0].union_many(daily[1:])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = trading_day_range()\n",
    "print('len(ix)=',len(ix))\n",
    "print(ix[20:40])\n",
    "print(ix[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trading_day_range(bday_start=None,bday_end=None,day_freq='B',\n",
    "#                       open_time='09:30',close_time='16:00',trade_freq='15T',weekmask=None):\n",
    "\n",
    "#ix1 = trading_day_range('01/01/2021 09:30','01/13/2021 16:00',day_freq='B',trade_freq='30T')#,weekmask='Wed Thu')\n",
    "ix1 = trading_day_range('01/01/2021 09:30','01/13/2021 16:00',bday_freq='C',iday_freq='30T',weekmask='Wed Thu Fri')\n",
    "print('len(ix1)=',len(ix1))\n",
    "print(ix1[20:40])\n",
    "print(ix1[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix2 = pd.bdate_range('01/01/2021 09:30','01/13/2021 16:00',freq='30T')\n",
    "#ix2 = pd.bdate_range('01/01/2021 09:30','01/13/2021 16:00',freq='C',weekmask=\"Mon Tue Wed Thu Fri\")\n",
    "ix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix2 = ix2[ix2.indexer_between_time(start_time='09:30',end_time='16:00')]\n",
    "ix2[20:40]\n",
    "ix2[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(ix1)\n",
    "len(ix2)\n",
    "bdays = pd.bdate_range('01/01/2021 09:30','01/13/2021 16:00',freq='B')\n",
    "\n",
    "\n",
    "ix2list = [d for d in ix2 if pd.Timestamp(d.date()) in bdays]\n",
    "#ix2list\n",
    "ix2n = pd.DatetimeIndex(ix2list)\n",
    "len(ix2n)\n",
    "ix2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(range(10),index=['A','B','C','D','E','F','G','H','I','J'])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[['A','C','F']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.DataFrame(range(10),index=pd.date_range('1/1/2021',periods=10))\n",
    "s = pd.DataFrame(range(10),index=pd.date_range('1/1/2021 09:30',periods=10,freq='12H'))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.index\n",
    "type(s.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s.loc[['2021-01-01','2021-01-04'],:]\n",
    "s.loc['2021-01-01']\n",
    "#s[['2021-01-01','2021-01-04']]\n",
    "#s['2021-01-01'] + s['2021-01-04']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['Date'] = [d.date() for d in s.index]\n",
    "s\n",
    "\n",
    "s['Date'][0]\n",
    "type(s['Date'][0])\n",
    "\n",
    "s['Date']\n",
    "s['Date'].isin([datetime.date(2021,1,1),datetime.date(2021,1,4)])\n",
    "\n",
    "s[ s['Date'].isin([datetime.date(2021,1,1),datetime.date(2021,1,4)]) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtix = pd.DatetimeIndex([datetime.date(2021,1,1),datetime.date(2021,1,4)])\n",
    "dtix\n",
    "dtix[0]\n",
    "dtix.to_pydatetime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['Date']\n",
    "s['Date'].isin([datetime.date(2021,1,1),datetime.date(2021,1,4)])\n",
    "s['Date'].isin([d.date() for d in dtix.to_pydatetime()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drbd = pd.date_range('01/01/2021 09:30','01/13/2021 16:00',freq=BD)\n",
    "# [d.date() for d in  drbd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dr = pd.date_range('01/01/2021 09:30','01/13/2021 16:00',freq='15T')\n",
    "# sd = pd.Series(range(len(dr)),index=dr)\n",
    "# sd\n",
    "# dlist = [d.date() for d in  drbd] \n",
    "# dlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm, lb = dit._lsq_linear(dtseries)\n",
    "lm, lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em, eb = dit._ep_linear(dtseries)\n",
    "em, eb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jj in range(len(dtseries)):\n",
    "    date = dtseries.iloc[jj]\n",
    "    \n",
    "    ly  = lm*mdates.date2num(date) + lb\n",
    "    ily = int(np.round(ly))\n",
    "    \n",
    "    ey  = em*mdates.date2num(date) + eb\n",
    "    iey = int(np.round(ey))\n",
    "    \n",
    "    print(\"%2d  %5.2f  %2d  %5.2f %20s\" % (ily,ly,iey,ey,date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldts = [dtseries[0] + pd.DateOffset(offset) for offset in range(30)]\n",
    "new_dtseries = pd.Series(alldts,index=pd.DatetimeIndex(alldts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eerr = []\n",
    "lerr = []\n",
    "print(\"jj ily  iey   ly     ey     dt  dt(il) dt(ie)\")\n",
    "for jj in range(len(new_dtseries)-1):\n",
    "    date = new_dtseries.iloc[jj]\n",
    "    \n",
    "    ly  = lm*mdates.date2num(date) + lb\n",
    "    ily = int(np.round(ly))\n",
    "    \n",
    "    ey  = em*mdates.date2num(date) + eb\n",
    "    iey = int(np.round(ey))\n",
    "    \n",
    "    ily = 0 if (ily < 0 or ily > 19) else ily\n",
    "    iey = 0 if (iey < 0 or iey > 19) else iey\n",
    "    \n",
    "    err_l = dtseries.iloc[ily].date().day - date.date().day\n",
    "    err_e = dtseries.iloc[iey].date().day - date.date().day\n",
    "    lerr.append(abs(err_l))\n",
    "    eerr.append(abs(err_e))\n",
    "\n",
    "    print(\"%2d  %2d  %2d  %5.2f  %5.2f  %4s  %4s  %4s  %4s  %4s\" % \n",
    "          (jj,ily,iey,ly,ey,date.date().day,dtseries.iloc[ily].date().day,\n",
    "           dtseries.iloc[iey].date().day, err_l, err_e)\n",
    "         )\n",
    "\n",
    "print('\\nsum(lerr)=',sum(lerr),' sum(eerr)=',sum(eerr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lerr = []\n",
    "eerr = []\n",
    "for jj in range(len(dtseries)):\n",
    "    date = dtseries.iloc[jj]\n",
    "    \n",
    "    ly  = lm*mdates.date2num(date) + lb\n",
    "    ily = int(np.round(ly))\n",
    "    \n",
    "    ey  = em*mdates.date2num(date) + eb\n",
    "    iey = int(np.round(ey))\n",
    "    \n",
    "    err_l = dtseries.iloc[ily].date().day - date.date().day\n",
    "    err_e = dtseries.iloc[iey].date().day - date.date().day\n",
    "    \n",
    "    lerr.append(abs(err_l))\n",
    "    eerr.append(abs(err_e))\n",
    "\n",
    "    \n",
    "    print(\"%2d  %5.2f  %2d  %5.2f %6s %6s %6s %6s %6s\" % (ily,ly,iey,ey,date.date().day,\n",
    "                                                  dtseries.iloc[ily].date().day,\n",
    "                                                  dtseries.iloc[iey].date().day,\n",
    "                                                  err_l, err_e))\n",
    "    \n",
    "print('\\nsum(lerr)=',sum(lerr),' sum(eerr)=',sum(eerr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
